{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_augment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uMTir00cUJC"
      },
      "source": [
        "# Adversarial AutoAugment\n",
        "\n",
        "Data augmentation (DA) has been widely utilized to improve generalization in training deep neural networks. Recently, human-designed data augmentation has been gradually replaced by automatically learned augmentation policy. \n",
        "\n",
        "<br/>\n",
        "\n",
        "UntangleAI's Adversarial AutoAugment simultaneously optimize target related object and augmentation policy search loss. The augmentation policy network attempts to increase the training loss of a target network through generating adversarial augmentation policies, while the target network can learn more robust features from harder examples to improve the generalization. It also manages Distributed Data Parallel to enable faster computations with no boilerplate code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EdrLEs2d2dz"
      },
      "source": [
        "## Training with Adversarial AutoAugment\n",
        "\n",
        "Example implementation that covers what is needed to effectively utilize UntangleAI's `train_augment` function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xdUoK6QeydG"
      },
      "source": [
        "#Required imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision.transforms import transforms\n",
        "import pickle\n",
        "from untangle import UntangleAI"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWVt_CiUfB6b"
      },
      "source": [
        "You can decide on what to use for optimizer and scheduler based on your network. Do not send in `None` for any of them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89jR661PfM6Z"
      },
      "source": [
        "from torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTWcVcnKfcZl"
      },
      "source": [
        "We use `TrainAugmentArgs` class to pass in our preferences.\n",
        "<br/>\n",
        "**A new directory 'mname' is created with sub-directory named train_augment/'experiment_ID'** \n",
        "*   `mname` is the name of the project\n",
        "*   `num_class` is the number of classes in your dataset\n",
        "*   `batch_size` is..well, batch size for the dataset. However, the batch size should be greater than your GPU count\n",
        "*    `epoch` -> Number of epochs to train the network\n",
        "*   `gpu_count` is the number of GPU's available for training. \n",
        "*    `experiment_ID` -> You can run the same model with different configurations by using different experment_ID for each of them. The results will be saved in appropriate folders\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "If some of your GPU's are being used by someone else, mask them using the command `export CUDA_VISIBLE_DEVICES=2,3,4` (masks 0 and 1 GPUs)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZNiOVKXfpca"
      },
      "source": [
        "class TrainAugmentArgs:\n",
        "    \"\"\"\n",
        "    A new directory 'mname' is created with sub-directory named train_augment/'experiment_ID' \n",
        "    \"\"\"\n",
        "    mname = 'test_net'\n",
        "    num_class = 10\n",
        "    batch_size = 128\n",
        "    epoch = 600\n",
        "    gpu_count = 8\n",
        "    experiment_ID = 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MRLKwpOjOTi"
      },
      "source": [
        "Your model goes here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXxk0NgXjWXh"
      },
      "source": [
        "class LeNet(nn.Module):\n",
        "    # TODO: This isn't really a LeNet, but we implement this to be\n",
        "    #  consistent with the Evidential Deep Learning paper\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.model = None\n",
        "        lenet_conv = []\n",
        "        lenet_conv += [nn.Conv2d(3, 6, 5)] \n",
        "        lenet_conv += [torch.nn.ReLU(inplace=True)]\n",
        "        lenet_conv += [nn.MaxPool2d(2, 2)]\n",
        "        lenet_conv += [nn.Conv2d(6, 16, 5)]\n",
        "        lenet_conv += [torch.nn.ReLU(inplace=True)]\n",
        "        lenet_conv += [nn.MaxPool2d(2, 2)]\n",
        "        lenet_dense = []\n",
        "        lenet_dense += [nn.Linear(16 * 5 * 5, 120)]\n",
        "        lenet_dense += [torch.nn.ReLU(inplace=True)]\n",
        "        lenet_dense += [nn.Linear(120, 84)]\n",
        "        lenet_dense += [torch.nn.ReLU(inplace=True)]\n",
        "        lenet_dense += [nn.Linear(84, 10)]\n",
        "\n",
        "        self.features = torch.nn.Sequential(*lenet_conv)\n",
        "        self.classifier = torch.nn.Sequential(*lenet_dense)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.features(input)\n",
        "        output = output.view(-1, 16 * 5 * 5)\n",
        "        output = self.classifier(output)\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_HhucnZjZVl"
      },
      "source": [
        "Everthing comes together here. Go through the code first and come back here.\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "\n",
        "1.   `if __name__ == '__main__':` **should be used** as we make use of [Distributed Data Parallel](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) by default.\n",
        "2.   No need to use `transforms.ToTensor` as train_augment handles it internally.`transforms.compose()` should be used for declaring transforms because we add the augmentations generated by the policy generator to these list of transforms along with `transforms.ToTensor`. \n",
        "3.   Empty `transforms.compose([])` should be used if there are no transforms needed except for `ToTensor()`. e.g. MNIST dataset\n",
        "4.   **train_augment** returns the trained model(best loss model)\n",
        "5.   Checkpoint models such as model at the end of each epoch are saved in 'mname'/train_augment/'experiment_ID'/models/ for custom experiments\n",
        "6.   Logs such as policies and losses are saved in 'mname'/train_augment/'experiment_ID'/logs.pkl\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYXIuVTZjgm0"
      },
      "source": [
        "if __name__ == '__main__': #1\n",
        "    args= TrainAugmentArgs()\n",
        "    untangleai = UntangleAI()\n",
        "    model = LeNet()\n",
        "    \n",
        "    transform_train = transforms.Compose([\n",
        "                transforms.RandomCrop(32, padding=4),transforms.RandomHorizontalFlip(),]) #2 explains why transforms are mandatory to use train_augment\n",
        "    \n",
        "    transform_test = transforms.Compose([]) #3 covers if there are no transforms\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./testDataset/', train=True, download=True,transform=transform_train,)\n",
        "    testset = torchvision.datasets.CIFAR10(root='./testDataset/', train=False, download=True,transform=transform_test)\n",
        "    optimizer = SGD(model.parameters(), lr = 0.01, momentum = 0.9, nesterov = True, weight_decay = 1e-4)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max = 2)\n",
        "\n",
        "    trained_model = untangleai.train_augment(model,optimizer,scheduler,trainset,testset,args) #4 explains what model is returned after training \n",
        "\n",
        "    #5 explains how to access checkpoint models -> Feel free to explore them\n",
        "    exp_path = untangleai.train_augment_path # test_net/train_augment/1/\n",
        "    \n",
        "    with open('test_net/train_augment/1/logs.pkl', 'rb') as f: #logs are saved at 'mname'/train_augment/'experiment_ID'/logs.pkl\n",
        "        data = pickle.load(f)\n",
        "        print(data) #6 explains what these logs are\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}